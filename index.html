<html>
    <head>
    <!-- Load TensorFlow.js. This is required to use MobileNet. -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.2.1"> </script>
    <!-- Load the MobileNet model. -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@1.0.0"> </script>
    <script
		src="https://code.jquery.com/jquery-3.4.0.min.js"
		integrity="sha256-BJeo0qm959uMBGb65z40ejJYGSgR7REI4+CW1fNKwOg="
		crossorigin="anonymous">
    </script>

    </head>
    <body>
        <div id="emotionContainer">
            <video autoplay="true" width="640" height="480" id="videoElement"></video>
            <p>Il mio stato d'animo: <span id ="emotion">Attendi un attimo</span></p>
        </div>
    </body>
</html>


<script>
let scoreThreshold = 0.5
var EmotionModel;
var emotion_labels = ["angry", "disgust", "fear", "happy", "sad", "surprise", "neutral"];
var emotion_colors = ["#ff0000", "#00a800", "#ff4fc1", "#ffe100", "#306eff", "#ff9d00", "#7c7c7c"];
var offset_x = 27;
var offset_y = 20;

// START VIDEO
startVideo();


 // LOAD FACE-API PER IL DETECT DEI VOLTI
 function loadFaceApi(){
    jQuery.getScript('/js/FaceEmotion/face-api.js', function(){
                    console.log('loaded FaceApi');
                    setTimeout(function(){
                        startFaceEmotion();
                    },4000); 
                });
 }




function startVideo() {
    // use MediaDevices API
    // docs:
    // https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia

    var video = document.querySelector('video'),canvas;
    webcamElement=video;
    console.log("navigator  " + navigator.mediaDevices)
    if (navigator.mediaDevices) {
        // access the web cam

        navigator.mediaDevices.getUserMedia({ video: true })
            // permission granted:
            .then(function(stream) {
             //video.src = window.URL.createObjectURL(stream);
		     video.srcObject = stream;
             loadFaceApi();
            })
            // permission denied:
            .catch(function(error) {
                document.body.textContent = 'Could not access the camera.Error:' + error.name;
            });
    }
}


// AVVIA IL LOADING DEL MODELLO PER LA VERIFICA DELLE EMOZIONI
function startFaceEmotion(){
    loadModel('/js/FaceEmotion/models/mobilenetv1_models/model.json');
}



// load emotion model
async function loadModel(path) {
    EmotionModel = await tf.loadLayersModel(path)
    console.log(EmotionModel);
    load_modello_di_riconoscimento();
}


async function load_modello_di_riconoscimento() {
    //const Model_url = '/js/FaceEmotion/models/tiny_face_detector/tiny_face_detector_model-weights_manifest.json'
    const Model_url = '/js/FaceEmotion/models/tiny_face_detector/'
    await faceapi.loadTinyFaceDetectorModel(Model_url);
    console.log("FaceEmotion modelLoaded");
    var video = document.querySelector('video'),canvas;
    start_riconoscimento(video);
}



async function start_riconoscimento(videoEl) {

    const {
        width,
        height
    } = faceapi.getMediaDimensions(videoEl)

    var canvas = canvas || document.createElement('canvas');
    canvas.width = width
    canvas.height = height

    const forwardParams = {
        inputSize: parseInt(160),
        scoreThreshold
    }

    const result = await faceapi.detectAllFaces(videoEl, new faceapi.TinyFaceDetectorOptions(forwardParams))
    // SE TROVO DEI VOLTI ESEGUO IL CONTROLLO DELL'EMOZIONE
    
    if (result.length != 0) {

        const context = canvas.getContext('2d')
        context.drawImage(videoEl, 0, 0, width, height)
        let ctx = context;
        for (var i = 0; i < result.length; i++) {
            ctx.beginPath();
            var item = result[i].box;
            let s_x = Math.floor(item._x+offset_x);
            if (item.y<offset_y){
                var s_y = Math.floor(item._y);
            }
            else{
                var s_y = Math.floor(item._y-offset_y);
            }
            let s_w = Math.floor(item._width-offset_x);
            let s_h = Math.floor(item._height);
            let cT = ctx.getImageData(s_x, s_y, s_w, s_h);
            cT = preprocess(cT);
            z = EmotionModel.predict(cT)
            if(z != "undefined" && z != null)
            {
                let index = z.argMax(1).dataSync()[0]
                let label = emotion_labels[index];
                var emotionEl = document.getElementById('emotion');
                emotionEl.innerHTML = label;
              console.log("risultato: "+label);
               
            }

        }
        await sleep(500);
    }

setTimeout(() => start_riconoscimento(videoEl));
}

async function loadNetWeights(uri) {
    return new Float32Array(await (await fetch(uri)).arrayBuffer())
}


function preprocess(imgData) {
    return tf.tidy(() => {
        let tensor = tf.browser.fromPixels(imgData).toFloat();
        tensor = tensor.resizeBilinear([100, 100])
        tensor = tf.cast(tensor, 'float32')
        const offset = tf.scalar(255.0);
        // Normalize the image 
        const normalized = tensor.div(offset);
        //We add a dimension to get a batch shape 
        const batched = normalized.expandDims(0)
        return batched
    })
}

function successCallback(stream) {
    var videoEl = $('#inputVideo').get(0)
    videoEl.srcObject = stream;
}

function errorCallback(error) {
    alert(error)
    console.log("navigator.getUserMedia error: ", error);
}

function sleep(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
}

</script>


<style>

#emotionContainer{
    font-size: 2em;
    padding:20px;
    text-align: center;
    
    
}
#emotion{
    color: brown;
}
</style>
